{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Strategy:\n",
    "    \n",
    "    def __init__(self, n_arms: int):\n",
    "        self.n_arms = n_arms\n",
    "        self.n_iters = 0\n",
    "        self.arms_states = np.zeros(n_arms)\n",
    "        self.arms_actions = np.zeros(n_arms)\n",
    "        \n",
    "    def flush(self):\n",
    "        self.n_iters = 0\n",
    "        self.arms_states = np.zeros(self.n_arms)\n",
    "        self.arms_actions = np.zeros(self.n_arms)\n",
    "        \n",
    "    def update_reward(self, arm: int, reward: int):\n",
    "        self.n_iters += 1\n",
    "        self.arms_states[arm] += reward\n",
    "        self.arms_actions[arm] += 1\n",
    "        \n",
    "    def choose_arm(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsGreedy(Strategy):\n",
    "    \n",
    "    def __init__(self, n_arms: int, eps: float = 0.1):\n",
    "        super().__init__(n_arms)\n",
    "        self.eps = eps\n",
    "        \n",
    "    def choose_arm(self):\n",
    "        \n",
    "        if random.random() < self.eps:\n",
    "            return random.randint(0, self.n_arms - 1)\n",
    "        else:\n",
    "            return np.argmax(self.arms_states / self.arms_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB1(Strategy):\n",
    "    \n",
    "    def choose_arm(self):\n",
    "        if self.n_iters < self.n_arms:\n",
    "            return self.n_iters\n",
    "        else:\n",
    "            return np.argmax(self.ucb())\n",
    "        \n",
    "        \n",
    "    def ucb(self):\n",
    "        ucb = self.arms_states / self.arms_actions\n",
    "        ucb += np.sqrt(2 * np.log(self.n_iters) / self.arms_actions)\n",
    "        return ucb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliEnv:\n",
    "    \n",
    "    def __init__(self, arms_proba: list):\n",
    "        self.arms_proba = arms_proba\n",
    "        \n",
    "    @property\n",
    "    def n_arms(self):\n",
    "        return len(self.arms_proba)\n",
    "        \n",
    "    def pull_arm(self, arm_id: int):\n",
    "        if random.random() < self.arms_proba[arm_id]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    \n",
    "    def __init__(self, env: BernoulliEnv, strategy: Strategy):\n",
    "        self.env = env\n",
    "        self.strategy = strategy\n",
    "        \n",
    "    def action(self):\n",
    "        arm = self.strategy.choose_arm()\n",
    "        reward = self.env.pull_arm(arm)\n",
    "        self.strategy.update_reward(arm, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_regret(env: BernoulliEnv, strategy: Strategy, n_iters=2000):\n",
    "    strategy.flush()\n",
    "    bandit = Bandit(env, strategy)\n",
    "    regrets = []\n",
    "    for i in range(n_iters):\n",
    "        reward = bandit.strategy.arms_actions.dot(env.arms_proba)\n",
    "        optimal_reward = np.max(env.arms_proba) * i\n",
    "        regret = optimal_reward - reward\n",
    "        regrets.append(regret)\n",
    "        bandit.action()\n",
    "        \n",
    "    return regrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Thompson(Strategy):\n",
    "    \n",
    "    def __init__(self, n_arms: int):\n",
    "    # your code here\n",
    "        \n",
    "    def update_reward(self, arm: int, reward: int):\n",
    "    # your code here \n",
    "        \n",
    "    def choose_arm(self):\n",
    "    # your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "be = BernoulliEnv([0.3, 0.5, 0.7])\n",
    "eps_1 = EpsGreedy(be.n_arms, 0.1)\n",
    "eps_2 = EpsGreedy(be.n_arms, 0.3)\n",
    "eps_3 = EpsGreedy(be.n_arms, 0.5)\n",
    "ucb = UCB1(be.n_arms)\n",
    "\n",
    "# следующая строка должна работать\n",
    "tompson = Thompson(be.n_arms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate regrets\n",
    "eps_regrets = calculate_regret(be, eps_1)\n",
    "eps_2_regrets = calculate_regret(be, eps_2)\n",
    "eps_3_regrets = calculate_regret(be, eps_3)\n",
    "ucb_regrets = calculate_regret(be, ucb)\n",
    "# следующая строка должна работать\n",
    "tompson_regrets = calculate_regret(be,tompson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример посылки в код для проверки\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Strategy:\n",
    "    \n",
    "    def __init__(self, n_arms: int):\n",
    "        self.n_arms = n_arms\n",
    "        self.n_iters = 0\n",
    "        self.arms_states = np.zeros(n_arms)\n",
    "        self.arms_actions = np.zeros(n_arms)\n",
    "        \n",
    "    def flush(self):\n",
    "        self.n_iters = 0\n",
    "        self.arms_states = np.zeros(self.n_arms)\n",
    "        self.arms_actions = np.zeros(self.n_arms)\n",
    "        \n",
    "    def update_reward(self, arm: int, reward: int):\n",
    "        self.n_iters += 1\n",
    "        self.arms_states[arm] += reward\n",
    "        self.arms_actions[arm] += 1\n",
    "        \n",
    "    def choose_arm(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    \n",
    "class Thompson(Strategy):\n",
    "    \n",
    "    def __init__(self, n_arms: int):\n",
    "    # your code here\n",
    "        \n",
    "    def update_reward(self, arm: int, reward: int):\n",
    "    # your code here \n",
    "        \n",
    "    def choose_arm(self):\n",
    "    # your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
