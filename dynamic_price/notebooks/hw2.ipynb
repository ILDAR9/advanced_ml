{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../data/hw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# считаем данные в соответствующие датафреймы\n",
    "FLD = \"/home/nur/projects/analysis/dynamic_price/data/hw2\"\n",
    "train_main_df = pd.read_csv(os.path.join(FLD, 'HW_train_main_data.csv'))\n",
    "train_additional_df = pd.read_csv(os.path.join(FLD, 'HW_train_additional_data.csv'))\n",
    "train_main_df['timestamp'] = pd.to_datetime(train_main_df['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_main_df = pd.read_csv(os.path.join(FLD, 'HW_test_main_data.csv'))\n",
    "test_additional_df = pd.read_csv(os.path.join(FLD, 'HW_test_additional_data.csv'))\n",
    "test_main_df['timestamp'] = pd.to_datetime(test_main_df['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим на колонки, информацию о пустых значениях и типах данных\n",
    "train_main_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_main_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Найти id топ-10 самых дорогих квартир из датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dict()\n",
    "r = train_main_df.nlargest(5, columns = 'full_sq')[['id', 'full_sq']]\n",
    "res[3] = r.id.values.tolist()\n",
    "res[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Построить зависимость средней стоимости квартиры от года и месяца продаж."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_main_df.timestamp = pd.to_datetime(train_main_df.timestamp)\n",
    "train_main_df['year'] = train_main_df.timestamp.dt.year\n",
    "train_main_df['month'] = train_main_df.timestamp.dt.month\n",
    "train_main_df['year_month'] = train_main_df['year'].astype(str) + '_' + train_main_df['month'].astype(str)\n",
    "train_main_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "train_main_df.groupby('year_month').mean().sort_values(by=['year', 'month']).plot(y=\"price\", figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Для каждой пары месяц-год найти индексы (не id) самых дорогих квартир."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res[2] = train_main_df[['year_month', 'price']].groupby('year_month').idxmax()['price'].astype(int).values.tolist()\n",
    "res[2][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Построить boxplot для цены для пар месяц-год. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 6))\n",
    "sns.boxplot(x='year_month', y='price', data=train_main_df.sort_values(by=['year', 'month']))\n",
    "plt.ylabel('price', fontsize=12)\n",
    "plt.xlabel('year_month', fontsize=12)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Найти id топ-5 самых больших квартир.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = train_main_df.nlargest(10, columns = 'price')[['id', 'price']]\n",
    "res[1] = r.id.values.tolist()\n",
    "res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Посчитать количество пропусков в life_sq.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[4] = train_main_df.life_sq.isnull().sum(axis=0)\n",
    "res[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Заполнить пропуски life_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# как вариант можно построить KNeighborsRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = KNeighborsRegressor()\n",
    "params = {'n_neighbors':[2,3,4,5,6,7,8,9,10], 'weights': ['uniform', 'distance']}\n",
    "grcv = GridSearchCV(clf, params, n_jobs=-1)\n",
    "grcv.fit(train_main_df[~train_main_df.life_sq.isnull()].full_sq.values.reshape(-1,1)\n",
    "         , train_main_df[~train_main_df.life_sq.isnull()].life_sq.values.reshape(-1,1))\n",
    "\n",
    "grcv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Сохранить коэффициенты корреляции Пирсона между (price, full_sq) и (price, life_sq без пропусков). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_main_df[['price', 'full_sq', 'life_sq']]\n",
    "df = df[~df.isnull()]\n",
    "price_full_sq = df.corrwith(df.full_sq, axis=0, drop=False, method='pearson')[0]\n",
    "price_life_sq = df.corrwith(df.life_sq, axis=0, drop=False, method='pearson')[0]\n",
    "res[5] = [price_full_sq, price_life_sq]\n",
    "res[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath_out = 'part1.csv'\n",
    "with open(fpath_out, 'w') as f:\n",
    "    for i in range(1, 6):\n",
    "        if type(res[i]) == list:\n",
    "            f.write(', '.join(map(str, res[i])))\n",
    "        else:\n",
    "            f.write(str(res[i]))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat part1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue Analysis & Data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_analysis(df):\n",
    "    missing_df = df.isnull().sum(axis=0).reset_index()\n",
    "    missing_df.columns = ['column_name', 'missing_count']\n",
    "    missing_df = missing_df.loc[(missing_df['missing_count'] > 0), :]\n",
    "    missing_df = missing_df.sort_values(by='missing_count')\n",
    "    ind = range(missing_df.shape[0])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    rects = ax.barh(ind, missing_df['missing_count'], color=\"blue\")\n",
    "    ax.set_yticks(ind)\n",
    "    ax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\n",
    "    ax.set_xlabel(\"Count of missing values\")\n",
    "    ax.set_title(\"Number of missing values in each column\")\n",
    "    plt.show()\n",
    "\n",
    "empty_analysis(train_main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_train_df = train_main_df.merge(train_additional_df, how='left', on='id')\n",
    "whole_test_df = test_main_df.merge(test_additional_df, how='left', on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot(colname, th = None):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    if th:\n",
    "        df = whole_train_df[whole_train_df[colname] >= th]\n",
    "    else:\n",
    "        df = whole_train_df\n",
    "    sns.boxplot(x=colname, y='price', data=df)\n",
    "    plt.ylabel('price', fontsize=12)\n",
    "    plt.xlabel(colname, fontsize=12)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.show()\n",
    "\n",
    "barplot('build_year', th = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fillna_const(df):\n",
    "#     consts = {\n",
    "#         'kitch_sq':-20,\n",
    "#               'hospital_beds_raion':0,\n",
    "#               'num_room':-20,\n",
    "# #               'materials':1\n",
    "#               }\n",
    "#     df.fillna(consts, inplace=True)\n",
    "\n",
    "# fillna_const(whole_train_df)\n",
    "# fillna_const(whole_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# def build_year_col(df):\n",
    "#     idx = (df.build_year < 1800) | (df.build_year > 2015)\n",
    "#     df.loc[idx, 'build_year'] = df[idx].max_floor.apply(lambda x: random.randint(1970, 2005) if x < 12 else random.randint(1998, 2007))\n",
    "    \n",
    "# build_year_col(whole_train_df)\n",
    "# build_year_col(whole_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавим дополнительные столбцы на основе имеющейся даты\n",
    "def date_newcol(df):\n",
    "    df['year'] = df.timestamp.dt.year\n",
    "\n",
    "    df['month'] = df.timestamp.dt.month\n",
    "\n",
    "    df['week_of_year'] = df.timestamp.dt.isocalendar().week.astype(int)\n",
    "\n",
    "    df['day_of_week'] = df.timestamp.dt.weekday\n",
    "\n",
    "    df['timestamp_int'] = df.timestamp.astype(int)\n",
    "\n",
    "    df['year_month'] = df['year'].astype(str) + '_' + df['month'].astype(str)\n",
    "    \n",
    "date_newcol(whole_train_df)\n",
    "date_newcol(whole_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def max_floor_col(df):\n",
    "#     idx = df.max_floor.isnull()\n",
    "#     df.loc[idx, 'max_floor'] = df[idx].floor.apply(lambda x: max(x+1, 15))\n",
    "\n",
    "# max_floor_col(whole_train_df)\n",
    "# max_floor_col(whole_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bins_col(df):\n",
    "    bins = [0, 30, 40, 50, 60, 70, 80, 90, 100, 200, 5326]\n",
    "    df['full_sq_bins'] = np.searchsorted(bins, df.full_sq.values)    \n",
    "    \n",
    "    bins = [0, 6, 10, 17, 26, 33, 46]\n",
    "    df['max_floor_bins'] = np.searchsorted(bins, df.max_floor.values)\n",
    "\n",
    "    bins = [0, 1917, 1950, 1960, 1978, 1991, 2000, 2006, 2011]\n",
    "    df['build_year_bins'] = np.searchsorted(bins, df.build_year.values)\n",
    "\n",
    "bins_col(whole_train_df)\n",
    "bins_col(whole_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_update(df):\n",
    "    idx = df.life_sq.isnull()\n",
    "    # добавим разность между общей и жилой площадью квартиры\n",
    "    df['pred_life_sq'] = np.NaN\n",
    "    no_life_sq = grcv.predict(df[idx].full_sq.values.reshape(-1,1))\n",
    "    df.loc[idx, 'pred_life_sq'] = list(no_life_sq.reshape(1,-1)[0])\n",
    "    \n",
    "    df['some_extra_sqr_2'] = df[\"full_sq\"] - df.life_sq.combine_first(df.pred_life_sq)\n",
    "    \n",
    "#     df.loc[idx, 'life_sq'] = df[idx].full_sq - df[idx].kitch_sq\n",
    "    df['some_extra_sqr'] = df[\"full_sq\"] - df[\"life_sq\"]\n",
    "    \n",
    "    assert df.some_extra_sqr_2.isnull().sum(axis=0) == 0\n",
    "#     assert df.life_sq.isnull().sum(axis=0) == 0\n",
    "#     assert df.kitch_sq.isnull().sum(axis=0) == 0\n",
    "#     assert df.some_extra_sqr.isnull().sum(axis=0) == 0\n",
    "\n",
    "area_update(whole_train_df)\n",
    "area_update(whole_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполним все пропуски константой\n",
    "whole_train_df.fillna(-20, inplace=True)\n",
    "whole_test_df.fillna(-20, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio_newcol(df):\n",
    "    # вспомним, что цена сильно зависит от площади квартиры, на основе этих данных\n",
    "    # добавим столбцы для отношения площадей\n",
    "    sigma = 1e-8\n",
    "    df[\"ratio_life_dash_full_sq\"] = df[\"life_sq\"] / (df[\"full_sq\"] + sigma)\n",
    "    df[\"ration_kitchen_dash_full_sq\"] = df[\"kitch_sq\"] / (df[\"full_sq\"] + sigma)\n",
    "    df[\"ration_extra_dash_full_sq\"] = df[\"some_extra_sqr\"] / (df[\"full_sq\"] + sigma)\n",
    "    df[\"floor_dash_max_floor\"] = df[\"floor\"] / (df[\"max_floor\"] + sigma)\n",
    "    df[\"avg_room_sq\"] = df['full_sq'] / (df.num_room + sigma)\n",
    "\n",
    "    # добавим воздраст здания\n",
    "    df['age'] = df[\"build_year\"] - df['year']\n",
    "\n",
    "ratio_newcol(whole_train_df)\n",
    "ratio_newcol(whole_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from currency_converter import CurrencyConverter\n",
    "\n",
    "def add_currency_col(df):\n",
    "    c = CurrencyConverter(fallback_on_missing_rate=True)\n",
    "    df['usd_dash_rub'] = df.timestamp.apply(lambda x: c.convert(1, 'USD', 'RUB', date=x))\n",
    "#     df['eur_dash_rub'] = df.timestamp.apply(lambda x: c.convert(1, 'EUR', 'RUB', date=x))\n",
    "    \n",
    "    dfg = df.groupby('year_month').usd_dash_rub.mean()\n",
    "    d = dict(zip(dfg.index, dfg.values))\n",
    "    df['usd_month'] = df.year_month.apply(d.get).astype(int)\n",
    "    \n",
    "#     dfg = df.groupby('year_month').eur_dash_rub.mean()\n",
    "#     d = dict(zip(dfg.index, dfg.values))\n",
    "#     df['eur_month'] = df.year_month.apply(d.get).astype(int)\n",
    "    \n",
    "add_currency_col(whole_train_df)\n",
    "add_currency_col(whole_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repair vals\n",
    "whole_train_df.loc[whole_train_df['apartment condition'] == 33, 'apartment condition'] = 3\n",
    "# Remove outliers full_sq\n",
    "idx = (whole_train_df.full_sq_bins == 4) & (whole_train_df.price > 1e8)\n",
    "whole_train_df.drop(whole_train_df[idx == True].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "whole_train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_train_df.life_sq.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_vals(df):\n",
    "    df['full_sq'] = np.log2(df.full_sq)\n",
    "#     df['population'] = np.log2(df.population)\n",
    "#     df['office_num'] = np.log2(df.office_num)\n",
    "#     df['life_sq'] = np.log2(df.life_sq)\n",
    "    \n",
    "    df.fillna(0, inplace=True)\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "#     df['full_sq'] = df.full_sq.astype(int)\n",
    "#     df['population'] = df.population.astype(int)\n",
    "#     df['office_num'] = df.office_num.astype(int)\n",
    "#     df['life_sq'] = df.life_sq.astype(int)\n",
    "    \n",
    "\n",
    "log_vals(whole_train_df)\n",
    "log_vals(whole_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = ['full_sq', 'life_sq', 'floor', 'max_floor',\n",
    "       'material', 'build_year', 'num_room', 'kitch_sq', 'apartment condition',\n",
    "       'sub_area', 'full_sq_bins', 'population', 'indust_part',\n",
    "       'preschool_facilities', 'school_facilities', 'hospital_beds_raion',\n",
    "       'healthcare_facilities', 'university_num', 'sport_objects_facilities',\n",
    "       'additional_education_facilities', 'culture_objects_facilities',\n",
    "       'shopping_centers_facilities', 'office_num', 'green_part', 'prom_part',\n",
    "       'cafe_count', 'church_facilities', 'mosque', 'leisure_facilities',\n",
    "       'year', 'month', 'week_of_year', 'day_of_week', 'timestamp_int',\n",
    "       'ratio_life_dash_full_sq','ration_kitchen_dash_full_sq',\n",
    "       'age', 'some_extra_sqr', 'some_extra_sqr_2',\n",
    "#             'ration_extra_dash_full_sq',\n",
    "            'floor_dash_max_floor',\n",
    "            'avg_room_sq', \n",
    "            'build_year_bins', \n",
    "            'max_floor_bins',\n",
    "#             'usd_month'\n",
    "           ]\n",
    "\n",
    "set(whole_train_df.columns) - set(col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not any(whole_train_df[col_list].isnull().sum(axis=0).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# теперь выберем для валидации случайные записи, а не деление по времени\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    whole_train_df[col_list],\n",
    "    whole_train_df.price, test_size=1425, random_state=42)\n",
    "# 1425"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_params = {\n",
    "#     'eta': 0.05,\n",
    "#     'max_depth': 4,\n",
    "#     'subsample': 0.7,\n",
    "#     'colsample_bytree': 0.7,\n",
    "#     'objective': 'reg:linear',\n",
    "#     'eval_metric': 'rmse',\n",
    "#     'min_child_weight':1,\n",
    "#     'n_estimators': 100,\n",
    "#     'silent': 1,\n",
    "#     'seed':0\n",
    "# }\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'min_child_weight':1,\n",
    "    'silent': 1,\n",
    "    'seed':0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train = xgb.DMatrix(X_train, y_train, feature_names = col_list, enable_categorical=True)\n",
    "xgb_test = xgb.DMatrix(X_test, y_test, feature_names = col_list, enable_categorical=True)\n",
    "evallist = [(xgb_test, 'eval'), (xgb_train, 'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = xgb.train(params = xgb_params, \n",
    "                    dtrain = xgb_train, \n",
    "                    num_boost_round = 2000, \n",
    "                    evals = evallist, \n",
    "                    early_stopping_rounds = 10, \n",
    "                    verbose_eval = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb_train_final = xgb.DMatrix(whole_train_df[col_list], whole_train_df.price, feature_names = col_list, enable_categorical=True)\n",
    "final_model = xgb.train(params = xgb_params, \n",
    "                    dtrain = xgb_train_final, \n",
    "                    num_boost_round = 880)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_val = xgb.DMatrix(whole_test_df[col_list], feature_names = col_list, enable_categorical=True)\n",
    "preds = final_model.predict(data=xgb_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id,predicted_price\n",
    "# 10,11.323928429511037\n",
    "\n",
    "fpath_out = 'prediction.csv'\n",
    "with open(fpath_out, 'w') as f:\n",
    "    f.write('id,predicted_price\\n')\n",
    "    for id_val, price in zip(whole_test_df.id.values, preds):\n",
    "        f.write(f\"{id_val},{price}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l prediction.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!explorer.exe .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_sq - самая важная, при этом падение в важности заметное\n",
    "# можно подумать над исправлением данного момента\n",
    "plot_importance(model,max_num_features=20, height=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_train_df['sub_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим на ошибки наших предсказаний\n",
    "\n",
    "scores = pd.DataFrame(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['predicted'] = model.predict(xgb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['error'] = scores.price - scores.predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['error'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# зная примеры, на которых большие ошибки, можно пробовать тюнить модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = scores.nlargest(20, columns = 'error').index\n",
    "whole_train_df[whole_train_df.index.isin(idxs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
